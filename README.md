# ğŸ”´ J.A.R.V.I.S. â€” Personal Knowledge Assistant

> **Just A Rather Very Intelligent System** â€” A next-generation RAG-powered document intelligence platform with Iron Man HUD aesthetics, semantic chunking, hybrid search, and temporal versioning.

![Python](https://img.shields.io/badge/Python-3.11-red?style=flat-square&logo=python&logoColor=white&labelColor=000)
![Streamlit](https://img.shields.io/badge/Streamlit-1.32+-red?style=flat-square&logo=streamlit&logoColor=white&labelColor=000)
![LangChain](https://img.shields.io/badge/LangChain-0.2+-red?style=flat-square&labelColor=000)
![License](https://img.shields.io/badge/License-MIT-red?style=flat-square&labelColor=000)

---

```
     â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•
     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
â–ˆâ–ˆ   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘
â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘
 â•šâ•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•  â•šâ•â•â•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•
```

---

## âœ¦ What is JARVIS?

JARVIS is a Personal Knowledge Assistant that transforms your documents into an intelligent, queryable knowledge base. It goes far beyond basic RAG with 7 advanced features:

| Feature | Technology | What it does |
|---|---|---|
| **Semantic Chunking** | `all-MiniLM-L6-v2` | Finds topic boundaries using cosine similarity â€” no more mid-sentence splits |
| **Hierarchical Summaries** | GPT-3.5/4o | 3-level tree: chunk â†’ section â†’ document summaries injected into prompts |
| **Temporal Versioning** | Custom diff engine | Tracks every document version, computes diffs, injects "as of" context |
| **Hybrid Search** | BM25 + FAISS + RRF | Keyword precision meets semantic understanding via Reciprocal Rank Fusion |
| **Multi-Query Fusion** | LangChain + GPT | Generates N query variants, retrieves with each, fuses into one answer |
| **Citation Highlighting** | GPT structured output | Maps every claim in the answer back to its source chunk with `[1]`, `[2]` markers |
| **Document Comparison** | Side-by-side RAG | Retrieves from 2+ documents independently, produces a structured comparison table |

---

## âœ¦ Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    INGESTION PIPELINE                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                    â•‘
â•‘  PDF / TXT / MD                                                    â•‘
â•‘       â”‚                                                            â•‘
â•‘       â–¼                                                            â•‘
â•‘  Extract Text (PyPDF2)                                             â•‘
â•‘       â”‚                                                            â•‘
â•‘       â–¼                                                            â•‘
â•‘  Sentence Tokenize                                                 â•‘
â•‘       â”‚                                                            â•‘
â•‘       â–¼                                                            â•‘
â•‘  Embed sentences (all-MiniLM-L6-v2)                               â•‘
â•‘       â”‚                                                            â•‘
â•‘       â–¼                                                            â•‘
â•‘  Windowed cosine similarity                                        â•‘
â•‘       â”‚                                                            â•‘
â•‘       â–¼                                                            â•‘
â•‘  Percentile breakpoint detection â”€â”€â–º SemanticChunk objects         â•‘
â•‘       â”‚                                                            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â•‘
â•‘  â”‚              â”‚                        â”‚                        â•‘
â•‘  â–¼              â–¼                        â–¼                        â•‘
â•‘ FAISS      BM25 Index             HierarchicalSummarizer          â•‘
â•‘ Index      (keyword)              chunkâ†’sectionâ†’document          â•‘
â•‘  â”‚              â”‚                        â”‚                        â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚                        â•‘
â•‘         â”‚                               â”‚                         â•‘
â•‘    TemporalVersionManager â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â•‘
â•‘    (version tracking + diffs)                                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                    QUERY PIPELINE                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                    â•‘
â•‘  User Question                                                     â•‘
â•‘       â”‚                                                            â•‘
â•‘       â”œâ”€â”€â”€ Standard RAG â”€â”€â–º FAISS retrieval â”€â”€â–º LLM               â•‘
â•‘       â”‚                                                            â•‘
â•‘       â”œâ”€â”€â”€ Hybrid Search â”€â”€â–º BM25 + FAISS â”€â”€â–º RRF â”€â”€â–º LLM        â•‘
â•‘       â”‚                                                            â•‘
â•‘       â””â”€â”€â”€ Multi-Query â”€â”€â–º N variants â”€â”€â–º N retrievals â”€â”€â–º        â•‘
â•‘                             Union â”€â”€â–º Fusion LLM                  â•‘
â•‘                                   â”‚                               â•‘
â•‘                                   â–¼                               â•‘
â•‘                         CitationHighlighter                       â•‘
â•‘                         (claim â†’ chunk mapping)                   â•‘
â•‘                                   â”‚                               â•‘
â•‘                                   â–¼                               â•‘
â•‘              Answer + [Citations] + Source passages               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## âœ¦ Quick Start

### Prerequisites
- Python 3.9+
- OpenAI API key ([get one here](https://platform.openai.com))

### 1. Clone the repo
```bash
git clone https://github.com/YOUR_USERNAME/jarvis-pka.git
cd jarvis-pka
```

### 2. Set up environment
```bash
python -m venv venv
source venv/bin/activate       # Windows: venv\Scripts\activate

# CPU-only (recommended for most machines & Streamlit Cloud)
pip install -r requirements-cpu.txt

# GPU machine (if you have CUDA)
pip install -r requirements.txt
```

### 3. Run JARVIS
```bash
# Option A â€” enter key in sidebar at runtime
streamlit run app.py

# Option B â€” set via environment variable
export OPENAI_API_KEY="sk-your-key"
streamlit run app.py
```

Open **http://localhost:8501** ğŸ”´

---

## âœ¦ Push to GitHub

```bash
# Initialize git (if not already done)
git init
git add .
git commit -m "feat: JARVIS PKA â€” semantic RAG, hybrid search, citations, versioning"

# Create a new repo at github.com then:
git remote add origin https://github.com/YOUR_USERNAME/jarvis-pka.git
git branch -M main
git push -u origin main
```

Your GitHub Actions CI will automatically run lint + tests on every push.

---

## âœ¦ Deploy to Streamlit Cloud (Free)

1. Push your code to GitHub (above)
2. Go to **[share.streamlit.io](https://share.streamlit.io)** and sign in with GitHub
3. Click **"New app"**
4. Select:
   - **Repository:** `YOUR_USERNAME/jarvis-pka`
   - **Branch:** `main`
   - **Main file path:** `app.py`
5. Click **"Advanced settings"** â†’ **Secrets** tab, add:
   ```toml
   OPENAI_API_KEY = "sk-your-key-here"
   ```
6. Under **"Packages"**, set the requirements file to `requirements-cpu.txt`
7. Click **Deploy** â€” live in ~3 minutes! ğŸš€

Your app will be at: `https://YOUR-APP-NAME.streamlit.app`

---

## âœ¦ Deploy to Hugging Face Spaces (Alternative â€” also free)

```bash
# Install HF CLI
pip install huggingface_hub

# Login
huggingface-cli login

# Create a space and push
git remote add hf https://huggingface.co/spaces/YOUR_HF_USERNAME/jarvis-pka
git push hf main
```

Then add `OPENAI_API_KEY` as a Secret in your Space's Settings page.

---

## âœ¦ Project Structure

```
jarvis-pka/
â”œâ”€â”€ app.py                          # Main JARVIS Streamlit application
â”œâ”€â”€ requirements.txt                # GPU dependencies
â”œâ”€â”€ requirements-cpu.txt            # CPU-only (for Streamlit Cloud)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ semantic_chunker.py         # â˜… Sentence-transformer breakpoint chunking
â”‚   â”œâ”€â”€ hierarchical_summarizer.py  # â˜… 3-level summary hierarchy
â”‚   â”œâ”€â”€ temporal_manager.py         # â˜… Document versioning & diff tracking
â”‚   â”œâ”€â”€ knowledge_base.py           # FAISS vector store with metadata
â”‚   â”œâ”€â”€ hybrid_search.py            # â˜… BM25 + FAISS + RRF hybrid retrieval
â”‚   â”œâ”€â”€ multi_query.py              # â˜… Multi-query generation & answer fusion
â”‚   â”œâ”€â”€ citation_comparator.py      # â˜… Citation highlighting & doc comparison
â”‚   â”œâ”€â”€ rag_chain.py                # Temporal-aware ConversationalRAG chain
â”‚   â””â”€â”€ document_loader.py          # PDF / TXT / MD text extraction
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_chunker.py
â”‚   â””â”€â”€ test_temporal.py
â”œâ”€â”€ .streamlit/config.toml          # JARVIS HUD theme config
â”œâ”€â”€ .github/workflows/ci.yml        # GitHub Actions CI
â””â”€â”€ .gitignore
```

---

## âœ¦ RAG Mode Comparison

| Mode | Best for | Extra cost |
|---|---|---|
| Standard RAG | Fast, everyday queries | None |
| Hybrid BM25+Semantic | Keyword-heavy docs (legal, technical) | None (local BM25) |
| Multi-Query Fusion | Complex, ambiguous questions | ~3Ã— LLM calls |

---

## âœ¦ Configuration

All parameters are tunable in the sidebar at runtime:

| Parameter | Default | Effect |
|---|---|---|
| Breakpoint sensitivity | 85 | Lower = more, smaller semantic chunks |
| Min/Max chunk tokens | 80/400 | Guards against micro/giant chunks |
| Smoothing window | 2 | Averages similarity over N neighbors |
| Retrieved chunks (k) | 6 | Chunks sent to LLM per query |
| Multi-query variants | 3 | Number of query reformulations |
| Semantic weight Î± | 0.5 | Balance between BM25 and semantic (0=BM25, 1=semantic) |
| Memory window | 5 | Past exchanges in context |

---

## âœ¦ License

MIT â€” use freely, attribution appreciated.

---

*Powered by LangChain Â· OpenAI Â· FAISS Â· sentence-transformers Â· Streamlit*
